---
description: "Cursor 2.4 agent orchestration: native subagents, Agent Skills, parallel workflows, hierarchical task decomposition, and multi-agent coordination"
alwaysApply: true
---

# Cursor 2.4 Agent Orchestration

Advanced patterns for working with Cursor 2.4's native subagents, Agent Skills system, and multi-agent coordination.

## Cursor 2.4 Architecture Overview

```
┌─────────────────────────────────────────────────────────────┐
│                    CURSOR 2.4 AGENT                         │
├─────────────────────────────────────────────────────────────┤
│  Main Agent                                                 │
│    ├── Built-in Subagents (automatic)                       │
│    │     ├── explore   → codebase search (fast model)       │
│    │     ├── bash      → shell commands (isolated output)   │
│    │     └── browser   → web automation (MCP-based)         │
│    │                                                        │
│    ├── Custom Subagents (.cursor/agents/)                   │
│    │     ├── verifier  → validate completed work            │
│    │     └── debugger  → error analysis                     │
│    │                                                        │
│    └── Context Sources                                      │
│          ├── Rules (.cursor/rules/) → always/glob-triggered │
│          ├── Skills (.cursor/skills/) → on-demand/slash cmd │
│          └── Semantic Index → meaning-based search          │
└─────────────────────────────────────────────────────────────┘
```

## GPT-5.2 Extra High Planning and Communication

Operate with high autonomy while keeping the Opus 4.5 structure:

- **Plan shape**: short phases with explicit verification steps; avoid long narratives.
- **Decision forks only**: ask questions only when choices materially change security, data integrity, or core architecture.
- **Default-first**: for reversible choices, state the assumption once and proceed.
- **Concise updates**: progress updates should be 1–2 sentences, high-signal only.
- **Tool-first execution**: prefer targeted tool calls, batch when possible.

## CRITICAL: Date-Aware Search Protocol

### Issue #3: Always Check Current Date First

**BEFORE any web search for tech stack, versions, or solutions:**

```
<think>
[DATE CONTEXT CHECK]

Current date from system: [extract from user_info or context]
Format for searches: [Month Year] - e.g., "January 2026"

This date MUST be used in all version/tech queries.
</think>
```

**Correct search format:**
```
✅ "Next.js latest stable version January 2026"
✅ "React 19 useFormStatus hook January 2026"
✅ "shadcn-ui installation guide 2026"

❌ "Next.js latest version {date_year}"  ← NO PLACEHOLDERS
❌ "React {version} hooks {month} {year}" ← NO TEMPLATE VARIABLES
❌ "Next.js latest version"              ← NO DATE = OUTDATED RESULTS
```

### Issue #5: No Template Placeholders in Searches

**NEVER use these patterns in web_search:**
- `{version}` 
- `{date_year}`
- `{month}`
- `${variable}`
- `[PLACEHOLDER]`

**ALWAYS use concrete values:**
```
<think>
Extracting current date: January 2026
Package: Next.js
Searching with actual values, not templates.
</think>

→ WebSearch(search_term="Next.js npm latest stable version January 2026")
```

---

## Issue #4: Auto Web-Search Error Recovery

### Persistent Error Detection

When the same error occurs 3+ times:

```
<think>
[ERROR PATTERN DETECTED]

Error: [exact error message]
Attempt count: [1/2/3/4+]
Previous fix attempts:
  - Attempt 1: [what was tried]
  - Attempt 2: [what was tried]
  - Attempt 3: [what was tried]

STATUS: 4th attempt reached → MANDATORY WEB SEARCH
</think>

→ WebSearch(search_term="[exact error message] solution [framework] [current month year]")
```

### Error Counter Protocol

```
<think>
[ERROR TRACKING]

Tracking errors by signature:
- Error signature: hash/fingerprint of error message
- Current count: [N]
- Threshold: 4

If count >= 4:
  ACTION: Stop trying same fixes, search for solutions online
  REASON: Internal knowledge insufficient for this specific issue
</think>
```

### Web Search Escalation Pattern

After 4th failed attempt:

```
// 1. Search for the exact error
→ WebSearch(search_term="[exact error text] fix [framework] [year]")

// 2. If no results, search for the error type
→ WebSearch(search_term="[error type] [framework] common causes [year]")

// 3. If still no results, search for the context
→ WebSearch(search_term="[what I was trying to do] [framework] tutorial [year]")

// 4. Use Context7 for official documentation (if available)
→ user-context7-resolve-library-id(libraryName="[framework]")
→ user-context7-query-docs(libraryId="...", query="[error-related-topic]")
```

---

## Issue #6: Hierarchical Task Decomposition

### EPIC-Based Todo Structure

For complex projects, use hierarchical task organization:

```
TODO Structure:
├── EPIC0: Prerequisites (design systems, dependencies, setup)
│   ├── EPIC0.1: Install core dependencies
│   ├── EPIC0.2: Configure design system
│   └── EPIC0.3: Set up project structure
├── EPIC1: Feature A (Auth)
│   ├── EPIC1.1: Create auth pages
│   ├── EPIC1.2: Create auth API routes
│   └── EPIC1.3: Add auth middleware
├── EPIC2: Feature B (Dashboard)
│   ├── EPIC2.1: Create dashboard layout
│   └── EPIC2.2: Add dashboard widgets
└── EPIC3: Integration & Testing
    ├── EPIC3.1: Integration testing
    └── EPIC3.2: E2E testing
```

### Todo Creation Rules

1. **Prerequisites First**: Always create EPIC0 for setup/dependencies
2. **Feature Isolation**: Each major feature gets its own EPIC
3. **Sub-task Granularity**: Each sub-task should be testable independently
4. **Sequential Dependencies**: Mark which EPICs depend on others

### Example Todo for "Build an Auth System":

```
todos: [
  { id: "EPIC0", content: "Prerequisites: Install dependencies and configure project", status: "in_progress" },
  { id: "EPIC0.1", content: "Install Next.js, Tailwind, shadcn-ui, NextAuth", status: "pending" },
  { id: "EPIC0.2", content: "Configure Prisma with database schema", status: "pending" },
  { id: "EPIC1", content: "Auth UI: Build all authentication pages", status: "pending" },
  { id: "EPIC1.1", content: "Create login page with form validation", status: "pending" },
  { id: "EPIC1.2", content: "Create register page with form validation", status: "pending" },
  { id: "EPIC1.3", content: "Create forgot password flow", status: "pending" },
  { id: "EPIC2", content: "Auth API: Build backend authentication", status: "pending" },
  { id: "EPIC2.1", content: "Create NextAuth configuration with providers", status: "pending" },
  { id: "EPIC2.2", content: "Create API routes for auth operations", status: "pending" },
  { id: "EPIC2.3", content: "Add middleware for protected routes", status: "pending" },
  { id: "EPIC3", content: "Testing: Verify each component works", status: "pending" },
  { id: "EPIC3.1", content: "Test login flow end-to-end", status: "pending" },
  { id: "EPIC3.2", content: "Test registration flow end-to-end", status: "pending" }
]
```

### Per-File Testing (Not Whole System)

**CRITICAL**: Test after each file, not after the whole feature:

```
<think>
[PER-FILE VERIFICATION]

Just created: src/app/login/page.tsx

Verification steps:
1. Check syntax with linter → ReadLints(paths=["src/app/login/page.tsx"])
2. Check imports resolve → run build for that file area
3. Check component renders → browser test if UI

DO NOT: Run full system tests yet
DO: Verify this single file works before moving to next
</think>
```

### Incremental Testing Protocol

```
After EACH file creation:
1. ReadLints(paths=["path/to/new/file"])
2. If UI component: Quick browser snapshot
3. If API route: Quick curl/fetch test
4. Mark sub-task complete only after file verified

After EACH EPIC completion:
1. Integration test for that EPIC only
2. Check for regressions in previous EPICs
3. Mark EPIC complete

After ALL EPICs:
1. Full system test
2. E2E testing
3. Mark project complete
```

---

## Minimal Questions Policy

Only ask when the decision is a **hard fork**:

- Security/auth flows, data deletion/migration, or major architectural changes.
- Irreversible choices with high cost to undo.

Otherwise:

- Choose a safe, reversible default.
- State the assumption once.
- Proceed and verify.

---

## Cursor 2.4 Native Subagents

Cursor 2.4 introduces **native subagents** - independent agents that handle discrete parts of tasks with their own context windows.

### Built-in Subagents (Automatic)

These subagents are used automatically by the agent when appropriate:

| Subagent | Purpose | Why It's a Subagent |
|----------|---------|---------------------|
| **explore** | Searches and analyzes codebases | Generates large intermediate output; uses fast model for parallel searches |
| **bash** | Runs series of shell commands | Command output is verbose; isolates logs from main context |
| **browser** | Controls browser via MCP tools | DOM snapshots are noisy; filters down to relevant results |

**You don't configure these** - they activate automatically based on task context.

### Custom Subagents

Define custom subagents in `.cursor/agents/` (project) or `~/.cursor/agents/` (user).

**File Format** (markdown with YAML frontmatter):

```markdown
---
name: security-auditor
description: Security specialist. Use when implementing auth, payments, or handling sensitive data.
model: fast
---

You are a security expert auditing code for vulnerabilities.

When invoked:
1. Identify security-sensitive code paths
2. Check for common vulnerabilities (injection, XSS, auth bypass)
3. Verify secrets are not hardcoded
4. Review input validation

Report findings by severity: Critical, High, Medium.
```

**Frontmatter Fields:**

| Field | Required | Description |
|-------|----------|-------------|
| `name` | No | Unique identifier (lowercase, hyphens). Defaults to filename. |
| `description` | No | When to use this subagent. Agent reads this to decide delegation. |
| `model` | No | `fast`, `inherit`, or specific model ID. Default: `inherit`. |
| `readonly` | No | If `true`, restricted write permissions. |
| `is_background` | No | If `true`, runs in background without blocking. |

### Invoking Custom Subagents

**Automatic**: Agent delegates based on task and description match.

**Explicit**: Use `/name` syntax in prompts:
```
> /verifier confirm the auth flow is complete
> /debugger investigate this error
```

### Task Tool (Different from Native Subagents)

The `Task` tool still exists but operates differently - it's for launching sub-agents programmatically:

```
→ Task(
    subagent_type="generalPurpose",  // or "explore"
    prompt="Check if authentication is already implemented",
    description="Check auth status",
    model="fast"
  )
```

**When to Use Task vs Native Subagents:**

| Use Case | Approach |
|----------|----------|
| Complex codebase exploration | Native `explore` (automatic) or Task with explore |
| Shell command sequences | Native `bash` (automatic) |
| Browser testing | Native `browser` (automatic) |
| Custom specialized work | Custom subagent in `.cursor/agents/` |
| Programmatic delegation | Task tool |

**Task Tool Best Practices:**
- Subagents start fresh - include ALL necessary context in prompt
- Use `model="fast"` for exploration and simple research
- Launch multiple Tasks in parallel for independent features
- Results are not visible to user - summarize in your response

---

### TodoWrite Tool - Task Management

Use `TodoWrite` for EPIC-based task tracking:

```
→ TodoWrite(
    todos=[
      { id: "EPIC0", content: "Setup: Install dependencies and configure project", status: "in_progress" },
      { id: "EPIC0.1", content: "Run npx create-next-app with TypeScript", status: "pending" },
      { id: "EPIC0.2", content: "Install and configure shadcn-ui", status: "pending" },
      { id: "EPIC1", content: "Feature: User Authentication", status: "pending" },
      { id: "EPIC1.1", content: "Create login page with form validation", status: "pending" },
      { id: "EPIC1.2", content: "Create register page", status: "pending" }
    ],
    merge=false  // Replace all todos
  )
```

**Updating Todo Status:**
```
→ TodoWrite(
    todos=[{ id: "EPIC0", status: "completed" }],
    merge=true  // Only update specified todos
  )
```

**When to Use TodoWrite:**
- Complex multi-step tasks (3+ steps)
- Tasks requiring careful planning
- User provides multiple tasks
- After receiving new instructions - capture as todos

**When NOT to Use:**
- Single straightforward tasks
- Tasks completable in < 3 steps
- Purely conversational requests

---

### CreatePlan Tool - Plan Mode

Use `CreatePlan` in Plan Mode for structured planning:

```
→ CreatePlan(
    name="Authentication System",
    overview="Implement complete auth with NextAuth.js, including login, register, and protected routes",
    plan="# Implementation Plan\n\n## Phase 1: Setup\n...",
    todos=[
      { id: "setup", content: "Install dependencies", status: "pending" },
      { id: "auth-pages", content: "Create auth pages", status: "pending" }
    ]
  )
```

**When to Use:**
- Large/ambiguous tasks requiring user approval
- Tasks with multiple valid approaches
- Architectural decisions needed

---

## Parallel Agent Patterns (Cursor 2.4)

### Cursor 2.4 Parallel Capabilities

- **Native Subagents**: Built-in explore, bash, browser run in parallel automatically
- **Custom Subagents**: Define specialized agents that run independently
- **Background Subagents**: Set `is_background: true` for non-blocking execution
- **Multi-Agent Worktrees**: Multiple agents in git worktrees for large features
- **Cloud Agents**: Long-running tasks in cloud environments

### When to Use Each Pattern

| Scenario | Approach |
|----------|----------|
| Codebase exploration | Native `explore` subagent (automatic) |
| Multiple shell commands | Native `bash` subagent (automatic) |
| Browser testing | Native `browser` subagent (automatic) |
| Validate completed work | Custom `verifier` subagent |
| Debug errors | Custom `debugger` subagent |
| Independent features | Parallel custom subagents or worktrees |
| Compare approaches | Multi-agent judging |

### Orchestrator Pattern with Custom Subagents

```
MAIN AGENT (Orchestrator):
├── Plans architecture
├── Delegates to custom subagents
├── Coordinates results
└── Runs /verifier for final check

SUBAGENT: verifier
├── Checks implementation exists
├── Runs tests
├── Validates edge cases
└── Reports pass/fail with details

SUBAGENT: debugger
├── Analyzes error
├── Identifies root cause
├── Suggests fix
└── Returns diagnosis
```

### Spec File Pattern (Still Useful)

For complex multi-agent work, create specs in `.cursor/specs/`:

```markdown
# .cursor/specs/feature-auth.md

## Feature: Authentication System

### Prerequisites
- Database configured with User model
- NextAuth.js installed
- Environment variables set

### Tasks
1. Create `/app/login/page.tsx` - Login form
2. Create `/app/register/page.tsx` - Registration form
3. Create `/app/api/auth/[...nextauth]/route.ts` - NextAuth config
4. Create `/middleware.ts` - Route protection

### Acceptance Criteria
- [ ] User can register with email/password
- [ ] User can login and receive session
- [ ] Protected routes redirect to login

### Verification
After completion, invoke: `/verifier confirm auth feature`
```

### Background Subagent Example

For long-running tasks, use `is_background: true`:

```markdown
---
name: test-runner
description: Test automation expert. Use proactively to run tests and fix failures.
is_background: true
---

You are a test automation expert.

When you see code changes, proactively run appropriate tests.
If tests fail, analyze and report findings.
```

---

## Cursor Hooks Integration

Hooks let you observe, control, and extend the agent loop using custom scripts. They run before or after defined stages of the agent loop.

### Hooks Configuration (`.cursor/hooks.json`)

**CRITICAL**: Use the correct format per [Cursor Hooks docs](https://cursor.com/docs/agent/hooks):

```json
{
  "version": 1,
  "hooks": {
    "stop": [
      { "command": "node .cursor/hooks/grind.js" }
    ],
    "beforeShellExecution": [
      { "command": ".cursor/hooks/audit.sh" }
    ]
  }
}
```

**Format Rules:**
- `version`: Must be `1` (required)
- `hooks`: Object with event names as keys (NOT an array)
- Each event contains an array of hook objects with `command` field

### Hook Events (Agent)

| Event | When | Use Case |
|-------|------|----------|
| `beforeShellExecution` | Before shell command | Block dangerous commands, require approval |
| `afterShellExecution` | After shell command | Audit logs, capture output |
| `beforeMCPExecution` | Before MCP tool call | Gate risky operations |
| `afterMCPExecution` | After MCP tool call | Log tool usage |
| `afterFileEdit` | After file edit | Auto-format, lint, validate |
| `beforeSubmitPrompt` | Before prompt sent | PII scanning, validation |
| `afterAgentResponse` | After agent message | Analytics, logging |
| `afterAgentThought` | After thinking block | Observe reasoning |
| `stop` | Agent loop ends | **RALPH loop**: Continue until goals met (tests pass, build succeeds) |

### Hook Events (Tab/Inline Completions)

| Event | When | Use Case |
|-------|------|----------|
| `beforeTabFileRead` | Before Tab reads file | Redact secrets |
| `afterTabFileEdit` | After Tab edits file | Format Tab edits |

### Example: Audit Hook

```bash
#!/bin/bash
# .cursor/hooks/audit.sh - Log all agent actions

json_input=$(cat)
timestamp=$(date '+%Y-%m-%d %H:%M:%S')
echo "[$timestamp] $json_input" >> /tmp/agent-audit.log
exit 0
```

### Example: Long-Running Agent Loop (RALPH Pattern)

The `stop` hook enables RALPH-style iterative improvement - agent keeps working until verification goals are met.

**Based on**: [Cursor agent best practices - long-running agent loop](https://cursor.com/blog/agent-best-practices#example-long-running-agent-loop)

```javascript
// .cursor/hooks/grind.js - RALPH loop implementation

const input = JSON.parse(await readStdin());
const { status, loop_count = 0 } = input;
const MAX_ITERATIONS = 5;

// Stop if aborted/errored or max iterations reached
if (status !== 'completed' || loop_count >= MAX_ITERATIONS) {
  console.log(JSON.stringify({}));
  process.exit(0);
}

// Check if goals are met (tests pass, build succeeds, etc.)
const goalsMet = checkGoals(); // Run npm test, npm run build, etc.

if (goalsMet) {
  // Goals met - stop the loop
  console.log(JSON.stringify({}));
} else {
  // Goals not met - continue with followup message
  console.log(JSON.stringify({
    followup_message: `Continue working. Iteration ${loop_count + 1}/${MAX_ITERATIONS}. Tests/build still failing.`
  }));
}
```

**How it works:**
1. Agent completes a task
2. `stop` hook runs and checks verification goals (tests, build, lint)
3. If goals met → return `{}` to stop
4. If goals not met → return `{ followup_message: "..." }` to continue
5. Agent receives followup and works on fixing issues
6. Loop continues until goals met or max iterations reached

**Configuration** (`.cursor/grind.json`):
```json
{
  "maxIterations": 5,
  "commands": ["npm test", "npm run build"],
  "stopOnSuccess": true
}
```

### Example: Block Git Commands Hook

```bash
#!/bin/bash
# .cursor/hooks/block-git.sh - Require gh CLI instead of git

input=$(cat)
command=$(echo "$input" | jq -r '.command // empty')

if [[ "$command" =~ ^git[[:space:]] ]]; then
  cat << EOF
{
  "permission": "deny",
  "user_message": "Git command blocked. Use gh CLI instead.",
  "agent_message": "The git command was blocked. Use gh tool for GitHub operations."
}
EOF
else
  echo '{"permission": "allow"}'
fi
```

### Hook Input/Output Schema

**Input (all hooks receive):**
```json
{
  "conversation_id": "string",
  "generation_id": "string", 
  "model": "string",
  "hook_event_name": "string",
  "cursor_version": "string",
  "workspace_roots": ["<path>"]
}
```

**Output (permission hooks):**
```json
{
  "permission": "allow" | "deny" | "ask",
  "user_message": "<shown to user>",
  "agent_message": "<sent to agent>"
}
```

**Output (stop hook - for RALPH loop):**
```json
{}  // Empty object = stop the loop

// OR continue with:
{
  "followup_message": "Continue working on the task. Fix the failing tests."
}
```

---

## Cursor 2.4 Agent Skills

Agent Skills are portable, version-controlled packages that teach agents domain-specific tasks. Skills are the preferred way to package specialized knowledge in Cursor 2.4.

### Skills vs Rules

| Feature | Rules (`.cursor/rules/`) | Skills (`.cursor/skills/`) |
|---------|--------------------------|----------------------------|
| Activation | `alwaysApply: true` or glob patterns | Auto-discovery or `/skill-name` |
| Format | Single `.mdc` file | Folder with `SKILL.md` + optional scripts |
| Executable | No | Yes - can include scripts |
| Progressive loading | No | Yes - loads resources on-demand |
| Best for | Always-on conventions, file-type patterns | Domain knowledge, procedures, how-tos |

### Skill Directory Structure

```
.cursor/skills/
└── deploy-app/
    ├── SKILL.md           # Required - main instructions
    ├── scripts/           # Optional - executable code
    │   ├── deploy.sh
    │   └── validate.py
    ├── references/        # Optional - detailed docs (loaded on-demand)
    │   └── REFERENCE.md
    └── assets/            # Optional - templates, configs
        └── config-template.json
```

### SKILL.md Format

```markdown
---
name: deploy-app
description: Deploy the application to staging or production. Use when deploying code or when user mentions deployment.
---

# Deploy App

Deploy the application using the provided scripts.

## Usage

Run the deployment script: `scripts/deploy.sh <environment>`

## Pre-deployment Validation

Before deploying, run: `python scripts/validate.py`
```

**Frontmatter Fields:**

| Field | Required | Description |
|-------|----------|-------------|
| `name` | Yes | Skill identifier. Must match folder name. |
| `description` | Yes | When to use. Agent reads this to determine relevance. |
| `license` | No | License name or file reference. |
| `disable-model-invocation` | No | If `true`, only invoked via `/skill-name`. |

### Invoking Skills

**Automatic**: Agent discovers and applies based on context + description.

**Explicit**: Type `/skill-name` in chat:
```
> /deploy-app deploy to staging
> /migrate-to-skills convert my rules
```

### Migration: Rules to Skills

Cursor 2.4 includes `/migrate-to-skills` to convert dynamic rules:

```
> /migrate-to-skills
```

This converts:
- **Dynamic rules** (`alwaysApply: false`, no globs) → Standard skills
- **Slash commands** → Skills with `disable-model-invocation: true`

**What NOT to migrate:**
- Rules with `alwaysApply: true` (keep as rules)
- Rules with `globs` patterns (keep as rules - skills don't have glob triggers)

### Recommended Organization

```
.cursor/
├── rules/                    # Keep these as rules
│   ├── minimax-m2-core.mdc          # alwaysApply: true
│   ├── cursor-agent-orchestration.mdc  # alwaysApply: true
│   ├── web-development.mdc          # globs: ["*.ts", "*.tsx"]
│   └── python-development.mdc       # globs: ["*.py"]
│
├── skills/                   # Domain knowledge, procedures
│   ├── deploy-production/
│   │   └── SKILL.md
│   ├── database-migration/
│   │   └── SKILL.md
│   └── security-audit/
│       └── SKILL.md
│
└── agents/                   # Custom subagents
    ├── verifier.md
    └── debugger.md
```

### Anti-Hallucination: Clear Descriptions

**Good** - Specific trigger conditions:
```yaml
description: "Deploy to production. Use when user mentions 'deploy', 'release', 'push to prod', or 'go live'."
```

**Bad** - Vague, will trigger inappropriately:
```yaml
description: "Helps with general deployment tasks"
```

---

## Summary: Cursor 2.4 Optimized Workflow

### Before Starting Any Task

```
1. CHECK DATE
   → Extract current date from context
   → Use in all web searches

2. CREATE EPIC TODO
   → EPIC0: Prerequisites
   → EPIC1-N: Features  
   → Each with sub-tasks (EPIC1.1, EPIC1.2, etc.)

3. FOR EACH SUB-TASK
   → Implement single file
   → Verify that file (lint, compile, render)
   → Mark sub-task complete
   → Continue to next

4. ON REPEATED ERRORS (4+)
   → Stop trying same fix
   → Web search for solution
   → Apply community solution
   → Continue

5. AFTER EPIC COMPLETE
   → Run /verifier to validate
   → Integration test for EPIC
   → Continue to next EPIC
```

### Cursor 2.4 Quality Gates

```
□ Date included in all version searches
□ No template placeholders in searches
□ EPIC-based todo structure created
□ Each file verified after creation
□ Web search triggered after 4th error
□ Custom subagents created for validation (.cursor/agents/)
□ Skills organized for domain knowledge (.cursor/skills/)
□ Hooks configured for automation
```

### Quick Reference: Subagents vs Skills vs Rules

| Need | Solution |
|------|----------|
| Always-on conventions | Rules with `alwaysApply: true` |
| File-type patterns | Rules with `globs` |
| Domain knowledge | Skills in `.cursor/skills/` |
| Specialized agents | Custom subagents in `.cursor/agents/` |
| Validation | `/verifier` subagent |
| Debugging | `/debugger` subagent |
